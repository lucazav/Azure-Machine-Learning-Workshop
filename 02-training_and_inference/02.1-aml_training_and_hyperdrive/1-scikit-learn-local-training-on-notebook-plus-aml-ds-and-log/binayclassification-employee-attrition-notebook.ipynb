{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn binary classification model training on local notebook. \n",
    "## Plus Azure ML Dataset converted to Pandas DataFrames.\n",
    "_**The code is plain vanilla Scikit-Learn training/creation of a Binary classification model.**_\n",
    "\n",
    "_**Azure ML is only used to gather original data from an AML Dataset.**_\n",
    "\n",
    "_**This notebook can run on a local PC or on any Azure ML Compute Instance or Azure ML VM.**_\n",
    "\n",
    "This code is the baseline for the next labs/notebooks in the Workshop moving to AML remote compute, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries to use in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from azureml.core import Workspace, Dataset\n",
    "from azureml.core import Environment\n",
    "\n",
    "# Check versions\n",
    "import azureml.core\n",
    "import sklearn\n",
    "import joblib\n",
    "import pandas\n",
    "\n",
    "print(\"Azure SDK version:\", azureml.core.VERSION)\n",
    "print('scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "print('joblib version is {}.'.format(joblib.__version__))\n",
    "print('pandas version is {}.'.format(pandas.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Workspace to load Tabular Datasets and log info from local training into AML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to an existing Azure ML Workshop in order to use Azure ML Datasets and Runs Logging into AML\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the IBM employee attrition data created before\n",
    "\n",
    "Note: as you are now accessing the workspace, the Notebook needs to be authenticated for access through device authentication. Hence, you will be prompted with a device login like so: \n",
    "\n",
    "    Performing interactive authentication. Please follow the instructions on the terminal.\n",
    "    To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code ARQPFR4B4 to authenticate.\n",
    "    \n",
    "Please follow these instructions in a new browser tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the IBM employee attrition dataset from the workspace\n",
    "attritionData = ws.datasets['IBM-Employee-Attrition'].to_pandas_dataframe()\n",
    "attritionData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the initial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Employee Count and Over18 as all values are 1 and hence attrition is independent of this feature\n",
    "attritionData = attritionData.drop(['EmployeeCount'], axis=1)\n",
    "attritionData = attritionData.drop(['Over18'], axis=1)\n",
    "\n",
    "# Dropping Employee Number since it is merely an identifier\n",
    "attritionData = attritionData.drop(['EmployeeNumber'], axis=1)\n",
    "\n",
    "# Since all values are 80 let's drop also StandardHours\n",
    "attritionData = attritionData.drop(['StandardHours'], axis=1)\n",
    "\n",
    "target = attritionData[\"Attrition\"]\n",
    "\n",
    "attritionXData = attritionData.drop(['Attrition'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split in Train and Test datasets (DataFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(attritionXData, \n",
    "                                                    target, \n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the categorical and numerical column names in separate lists\n",
    "categorical = []\n",
    "for col, value in attritionXData.iteritems():\n",
    "    if value.dtype == 'object':\n",
    "        categorical.append(col)\n",
    "        \n",
    "numerical = attritionXData.columns.difference(categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform raw features\n",
    "We can explain raw features by either using a sklearn.compose.ColumnTransformer or a list of fitted transformer tuples. The cell below uses sklearn.compose.ColumnTransformer. In case you want to run the example with the list of fitted transformer tuples, comment the cell below and uncomment the cell that follows after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data processing pipelines (Scikit-Learn pipelines)\n",
    "NOTE: This code uses Scikit-Learn pipelines. Not related to AML Pipelines. Different concept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# We create the transformations pipelines for both numeric and categorical data.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "transforms_pipeline = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical),\n",
    "        ('cat', categorical_transformer, categorical)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add classifier algorithm (SVC: Support Vector Classifier) to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append classifier to Scikit-Learn transformations pipeline.\n",
    "# Now we have a full Scikit-Learn prediction pipeline.\n",
    "model_pipeline = Pipeline(steps=[('preprocessor', transforms_pipeline),\n",
    "                      ('classifier', SVC(kernel='linear', C = 1.0, probability=True))]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AML Experiment, run and log just for logging info while training locally in Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start an interactive logging session using *start_logging()*. It'll create also an interactive run in the specified experiment. Any metrics logged during the session are added to the run record in the experiment. The method *run.complete()* ends the sessions and marks the run as completed.\n",
    "You can find other logging examples [here](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/track-and-monitor-experiments/logging-api/logging-api.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "# Get an experiment object from AML\n",
    "experiment = Experiment(workspace=ws, name=\"aml-wrkshp-local-train-notebook-log\")\n",
    "\n",
    "# Create an interactive run object in the experiment\n",
    "run =  experiment.start_logging()\n",
    "\n",
    "# Log the algorithm parameter C to the run\n",
    "run.log('C', 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the SVM (Support Vector Machine) Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_pipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions and calculate Accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "\n",
    "# Make Multiple Predictions\n",
    "y_predictions = model.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_predictions)\n",
    "rocauc = roc_auc_score(y_test, y_predictions)\n",
    "average_precision = average_precision_score(y_test, y_predictions)\n",
    "\n",
    "model_details_df = pd.DataFrame([accuracy, rocauc, average_precision],\n",
    "                                columns = ['SVM'],\n",
    "                                index=['Accuracy','ROC-AUC','Avg Precision'])\n",
    "\n",
    "model_details_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred_proba = model.predict_proba(x_test)[::,1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
    "\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(rocauc))\n",
    "plt.legend(loc=4)\n",
    "run.log_image(\"ROC Curve\", plot=plt)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log metric and model into the AML run definition\n",
    "### Note that training is local, we just use the run definition to log information about the run/training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current folder, where the pkl file will be created\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the Mean Squared Error to the notebook and to the run\n",
    " \n",
    "run.log('Accuracy', accuracy)\n",
    "run.log('ROC-AUC', rocauc)\n",
    "run.log('Avg Precision', average_precision)\n",
    "\n",
    "\n",
    "# Save the model to the outputs directory for capture\n",
    "model_file_name = 'outputs/model.pkl'\n",
    "\n",
    "# Allow local file overwriting\n",
    "model_file_obj = open(model_file_name, 'wb')\n",
    "\n",
    "# Persiste the model into the pkl file\n",
    "joblib.dump(value = model, filename = model_file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any file into the ./output folder will be automatically uploaded into the run artifacts \n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In case of duplicate artifact errors\n",
    "# run.cancel() # Even if an exception is thrown, the run will be canceled anyway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the experiment run and its logged info in Azure ML Workspace\n",
    "Now, you should go to your AML Workspace and check the information logged for this run, such as the accuracy, hyper-parameters and any other info you logged for the experiment run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easly get the values of the confusion matrix associated to the model in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "class_names = y_test.unique()\n",
    "# Just check that y_test contains all the classification label in the target variable\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [(\"Confusion matrix, without normalization\", None),\n",
    "                  (\"Normalized confusion matrix\", 'true')]\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(model, x_test, y_test,\n",
    "                                 display_labels=class_names,\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the instance you want to use as input for a prediction\n",
    "instance_num = 6 # The seventh instance from the beginning (0-based)\n",
    "\n",
    "# Get the prediction for the upon defined index\n",
    "prediction_values = model.predict(x_test)\n",
    "prediction_probs = model.predict_proba(x_test)\n",
    "\n",
    "print(\"Classes:\")\n",
    "print(model.classes_)\n",
    "\n",
    "print(\"Prediction label for instance\", instance_num, \":\")\n",
    "print(prediction_values[instance_num])\n",
    "\n",
    "print(\"True label for instance\", instance_num, \":\")\n",
    "print(y_test.values[instance_num])\n",
    "\n",
    "print(\"Prediction probabilities for instance\", instance_num, \":\")\n",
    "print(prediction_probs[instance_num])\n",
    "\n",
    "x_test.iloc[instance_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_probs[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_values[0:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I expected the default used threshold was 0.5. If so the 7th predicted label would be 0.\n",
    "\n",
    "I found [here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) that *predict_proba* may be inconsistent with *predict*, as the probabilities are calibrated using Platt scaling (more details [here](https://scikit-learn.org/stable/modules/svm.html#scores-probabilities)).\n",
    "\n",
    "Out of curiosity, I'm looking for the possible threhsolds to be used to make predicted values consistent with probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tr in np.arange(0.0, 1.0, 0.01):\n",
    "    if all( np.where(prediction_probs[:,1] > tr, True, False) == prediction_values ):\n",
    "        print('Possible threshold to use:', tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
