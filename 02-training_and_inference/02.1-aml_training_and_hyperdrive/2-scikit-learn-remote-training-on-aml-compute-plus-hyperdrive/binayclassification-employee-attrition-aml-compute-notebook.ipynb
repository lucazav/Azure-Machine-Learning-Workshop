{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remote Training Via Azure ML Compute (AML Cluster) and HyperDrive (Hyper-parameter Tuning with Multiple Children Runs) \n",
    "_**This notebook showcases the creation of a ScikitLearn Binary classification model by remotely training on Azure ML Compute Target (AMLCompute Cluster).**_\n",
    "\n",
    "_**It shows multiple ways of remote training like using a single Estimator, a ScriptRunConfig and hyper-parameter tunning with HyperDrive with multiple child trainings**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check library versions\n",
    "This is important when interacting with different executions between remote compute environments (cluster) and the instance/VM with the Jupyter Notebook.\n",
    "If not using the same versions you can have issues when creating .pkl files in the cluster and downloading them to load it in the Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check versions\n",
    "import azureml.core\n",
    "import sklearn\n",
    "import joblib\n",
    "import pandas\n",
    "\n",
    "print(\"Azure SDK version:\", azureml.core.VERSION)\n",
    "print('scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "print('joblib version is {}.'.format(joblib.__version__))\n",
    "print('pandas version is {}.'.format(pandas.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and connect to AML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create An Experiment\n",
    "\n",
    "**Experiment** is a logical container in an Azure ML Workspace. It hosts run records which can include run metrics and output artifacts from your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment_name = 'aml-wrkshp-remote-training-amlcompute'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to AmlCompute\n",
    "\n",
    "Azure Machine Learning Compute is managed compute infrastructure that allows the user to easily create single to multi-node compute of the appropriate VM Family. It is created **within your workspace region** and is a resource that can be used by other users in your workspace. It autoscales by default to the max_nodes, when a job is submitted, and executes in a containerized environment packaging the dependencies as specified by the user. \n",
    "\n",
    "Since it is managed compute, job scheduling and cluster management are handled internally by Azure Machine Learning service. \n",
    "\n",
    "For more information on Azure Machine Learning Compute Targets, please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-target)\n",
    "\n",
    "**Note**: As with other Azure services, there are limits on certain resources (for eg. AmlCompute quota) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota.\n",
    "\n",
    "Here a picture which explains the architecture behind Azure ML remote training:\n",
    "\n",
    "![](img/aml-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create project directory and copy the training script into the project directory\n",
    "\n",
    "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script, and any additional files your training script depends on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = './classif-attrition-amlcompute'\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "\n",
    "# Copy the training script into the project directory\n",
    "shutil.copy('train.py', project_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect or Create a Remote AML compute cluster\n",
    "\n",
    "Try to use the compute target you had created before (make sure you provide the same name here in the variable `cpu_cluster_name`).\n",
    "If not available, create a new cluster from the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS3_v2',\n",
    "                                                           max_nodes=10)\n",
    "    compute_target = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)\n",
    "    \n",
    "# For a more detailed view of current AmlCompute status, use get_status()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the AML Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_dataset = ws.datasets['IBM-Employee-Attrition']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Environment \n",
    "\n",
    "Azure Machine Learning environments are an encapsulation of the environment where your machine learning training happens. They specify the Python packages, environment variables, and software settings around your training and scoring scripts. They also specify run times (Python, Spark, or Docker). The environments are managed and versioned entities within your Machine Learning workspace that enable reproducible, auditable, and portable machine learning workflows across a variety of compute targets.\n",
    "\n",
    "You can use an Environment object on your local compute to:\n",
    "\n",
    "* Develop your training script.\n",
    "* Reuse the same environment on Azure Machine Learning Compute for model training at scale.\n",
    "* Deploy your model with that same environment.\n",
    "* Revisit the environment in which an existing model was trained.\n",
    "\n",
    "Read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/concept-environments) for more details.\n",
    "\n",
    "#### List all the curated environments and packages in your AML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "envs = Environment.list(workspace=ws)\n",
    "\n",
    "# List Environments and packages in my workspace\n",
    "for env in envs:\n",
    "    if env.startswith(\"AzureML\"):\n",
    "        print(\"Name\",env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use curated environment from AML named \"AzureML-Tutorial\"\n",
    "curated_environment = Environment.get(workspace=ws, name=\"AzureML-Tutorial\")\n",
    "\n",
    "# Get environment's details\n",
    "print(\"packages\", curated_environment.python.conda_dependencies.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Custom Environment (optional)\n",
    "\n",
    "You can also start building your custom environment using a curated environment as baseline. You have to save curated environment definition files into a folder and then edit them according to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save curated environment definition to folder (Two files, one for conda_dependencies.yml and another file for azureml_environment.json)\n",
    "curated_environment.save_to_directory(path=\"./curated_environment_definition\", overwrite=True)\n",
    "\n",
    "# Create custom Environment from Conda specification file\n",
    "custom_environment = Environment.from_conda_specification(name=\"custom-workshop-environment\", file_path=\"./curated_environment_definition/conda_dependencies.yml\")\n",
    "\n",
    "# Save curated environment definition to folder (Two files, one for conda_dependencies.yml and another file for azureml_environment.json)\n",
    "custom_environment.save_to_directory(path=\"./custom_environment_definition\", overwrite=True)\n",
    "\n",
    "custom_environment.register(ws)\n",
    "\n",
    "envs = Environment.list(workspace=ws)\n",
    "\n",
    "# List Environments and packages in my workspace\n",
    "for env in envs:\n",
    "    if env.startswith(\"custom\"):\n",
    "        print(\"Environment Name\",env)\n",
    "        print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Option A) Configure & Run using ScriptRunConfig & Environment \n",
    "\n",
    "### Easiest path using curated environments, but less flexible than Estimator\n",
    "\n",
    "The executed \"run\" will be a *ScriptRun* object, which can be used to monitor the asynchronous execution of the run, log metrics and store output of the run, and analyze results and access artifacts generated by the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add training script to run config\n",
    "from azureml.core import ScriptRunConfig, RunConfiguration, Experiment\n",
    "\n",
    "# # First run\n",
    "# script_runconfig = ScriptRunConfig(\n",
    "#     source_directory=project_folder,\n",
    "#     script=\"train.py\",\n",
    "#     arguments=[aml_dataset.as_named_input('attrition')]\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# # Second run\n",
    "# solver   = 'saga'\n",
    "# penalty  = 'elasticnet'\n",
    "# l1_ratio = '0.2'\n",
    "\n",
    "# Third run\n",
    "solver   = 'saga'\n",
    "penalty  = 'elasticnet'\n",
    "l1_ratio = '0.3'\n",
    "\n",
    "script_runconfig = ScriptRunConfig(\n",
    "    source_directory=project_folder,\n",
    "    script=\"train.py\",\n",
    "    arguments=[aml_dataset.as_named_input('attrition'), '--solver', solver, '--penalty', penalty, '--l1_ratio', l1_ratio]\n",
    ")\n",
    "\n",
    "# Attach compute target to run config\n",
    "# Use runconfig.run_config.target = \"local\" to exec the run in your current environment\n",
    "script_runconfig.run_config.target = compute_target\n",
    "\n",
    "\n",
    "# Attach environment to run config\n",
    "script_runconfig.run_config.environment = curated_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how the run_config JSON settings look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_runconfig.run_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's run the training script on the AML Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the Experiment Run to the AML Compute\n",
    "run = experiment.submit(script_runconfig)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor your Run using the Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get log results upon completion\n",
    "Model training and monitoring happen in the background. Wait until the model has finished training before you run more code. Use *wait_for_completion* to show when the model training is finished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can execute this section again using the other setups above in order to have more than one logged models\n",
    "\n",
    "\n",
    "## (Option B.1) Configure an Estimator with specific pkgs versions (using pip and conda)\n",
    "\n",
    "### Risky! Overriding remote compute Docker image packages with pip and conda might cause issues with inconsistent package versions.\n",
    "\n",
    "*Estimator* represents a generic estimator to train data using any supplied framework. Compared to a ScriptRunConfig, which only admits Environment objects as a parameter, the Estimator can directly receive lists of dependencies as a parameters.\n",
    "\n",
    "This class is designed for use with machine learning frameworks that do not already have an Azure Machine Learning pre-configured estimator. Pre-configured estimators exist for *[Chainer](https://chainer.org/)*, *PyTorch*, *TensorFlow* and *SKLearn*.\n",
    "\n",
    "The SKLearn pre-configured estimator is used for training in Scikit-learn experiments. When submitting a training job, Azure ML runs your script in a conda environment within a Docker container. SKLearn containers have the following dependencies installed:\n",
    "\n",
    "* Python (3.6.2)\n",
    "* azureml-defaults (Latest)\n",
    "* IntelMpi (2018.3.222)\n",
    "* scikit-learn (0.20.3)\n",
    "* numpy (1.16.2)\n",
    "* miniconda (4.5.11)\n",
    "* scipy (1.2.1)\n",
    "* joblib (0.13.2)\n",
    "* git (2.7.4)\n",
    "\n",
    "Except for the SKLearn one, estimators support single-node as well as multi-node execution. A distributed training job can be run using, for example, Message Passing Interface (MPI) objects. Obviously, a distributed training job can be run:\n",
    "\n",
    "* only upon the Compute Target Cluster\n",
    "* only if the used libraries provide distributed backends.\n",
    "\n",
    "The backends distributed training supported by Azure ML are:\n",
    "\n",
    "* Message Passing Interface (MPI)\n",
    "* NVIDIA Collective Communication Library (NCCL)\n",
    "* Gloo\n",
    "\n",
    "An example of Fast.AI distributed training [here](https://github.com/nash-lian/Distributed-training-Image-segmentation-Azure-ML)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "from azureml.train.sklearn import SKLearn\n",
    "\n",
    "script_params = {\n",
    "    \"--solver\": 'saga',\n",
    "    \"--penalty\": 'elasticnet',\n",
    "    \"--l1_ratio\": 0.4\n",
    "}\n",
    "\n",
    "pip_packages = [\n",
    "                'azureml-core==1.17.0', 'azureml-telemetry==1.17.0', 'azureml-dataprep==2.4.2',\n",
    "                'joblib==0.14.1', 'pandas==1.0.0', 'sklearn-pandas==2.0.2' \n",
    "               ]\n",
    "\n",
    "# Using plain Estimator class\n",
    "estimator = Estimator(source_directory=project_folder, \n",
    "                      script_params=script_params,\n",
    "                      compute_target=compute_target,\n",
    "                      entry_script='train.py',\n",
    "                      pip_packages=pip_packages,\n",
    "                      conda_packages=['scikit-learn==0.22.2.post1'],\n",
    "                      inputs=[ws.datasets['IBM-Employee-Attrition'].as_named_input('attrition')])\n",
    "\n",
    "\n",
    "# # Using SKLearn estimator class\n",
    "# estimator = SKLearn(source_directory=project_folder, \n",
    "#                     script_params=script_params,\n",
    "#                     compute_target=compute_target,\n",
    "#                     entry_script='train.py',\n",
    "#                     pip_packages=pip_packages,\n",
    "#                     conda_packages=['scikit-learn==0.22.2.post1'],\n",
    "#                     inputs=[aml_dataset.as_named_input('attrition')])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(estimator)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Option B.2) Configure an Estimator with Environment\n",
    "\n",
    "### Better! Easier! Consistent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an Estimator with Curated Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "# # Load Custom Environment from Workspace\n",
    "# custom_environment = Environment.get(workspace=ws,name=\"custom-workshop-environment\")  # ,version=\"1\"\n",
    "# print(custom_environment)\n",
    "\n",
    "script_params = {\n",
    "    '--solver': 'liblinear',\n",
    "    '--penalty': 'l2'\n",
    "}\n",
    "\n",
    "# Using plain Estimator class with custom Environment\n",
    "estimator = Estimator(source_directory=project_folder, \n",
    "                      script_params=script_params,\n",
    "                      compute_target=compute_target,\n",
    "                      entry_script='train.py',\n",
    "                      environment_definition=curated_environment,\n",
    "                      #environment_definition=custom_environment,\n",
    "                      inputs=[ws.datasets['IBM-Employee-Attrition'].as_named_input('attrition')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(estimator)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you need to cancel a run, you can follow [these instructions](https://aka.ms/aml-docs-cancel-run)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Option C) Configure and Run with Intelligent hyperparameter tuning (HyperDrive using Estimator)\n",
    "\n",
    "IMPORTANT: You need to have created either an *Estimator* or an *ScriptRunConfig* in the previous steps. \n",
    "\n",
    "The adjustable parameters that govern the training process are referred to as the **hyperparameters** of the model. The goal of hyperparameter tuning is to search across various hyperparameter configurations and find the configuration that results in the best performance.\n",
    "\n",
    "To demonstrate how Azure Machine Learning can help you automate the process of hyperarameter tuning, we will launch multiple runs with different values for numbers in the sequence. First let's define the parameter space using random sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a hyperparameter sweep\n",
    "First, we will define the hyperparameter space to sweep over. \n",
    "In this example we will use random sampling to try different configuration sets of hyperparameters to maximize our primary metric, Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import RandomParameterSampling, BayesianParameterSampling \n",
    "from azureml.train.hyperdrive import BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive import choice, uniform\n",
    "    \n",
    "# Values for \"solver\": {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, default='lbfgs'\n",
    "# Values for \"penalty\": {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
    "# Note that some penalty parameters are not supported by some algorithms. For example, \n",
    "param_sampling = RandomParameterSampling( {\n",
    "    \"--C\": uniform(0.0, 1.0),\n",
    "    \"--solver\": choice('newton-cg', 'lbfgs', 'sag', 'saga'),\n",
    "    \"--penalty\": choice('none', 'l2')\n",
    "    }\n",
    ")\n",
    "\n",
    "# Details on Scikit-Learn LogisticRegression hyper-parameters:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define an early termination policy. The *BanditPolicy* basically states to check the job every 2 iterations. If the primary metric (defined later) falls outside of the top 10% range, Azure ML terminate the job. This saves us from continuing to explore hyperparameters that don't show promise of helping reach our target metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_termination_policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)\n",
    "\n",
    "# Note that early termination policy is currently NOT supported with Bayesian sampling\n",
    "# Check here for recommendations on the multiple policies:\n",
    "# https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters#picking-an-early-termination-policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to configure a run configuration object, and specify the primary metric 'AUC_weighted' that's recorded in your training runs. \n",
    "If you go back to visit the training script, you will notice that this value is being logged. \n",
    "We also want to tell the service that we are looking to maximizing this value. \n",
    "We also set the number of samples to 20, and maximal concurrent job to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that in this case when using HyperDrive, we are using the script_runconfig configurations,\n",
    "# and not the original Estimator's parameters. You can only use one of the two configurationse \n",
    "hyperdrive_config = HyperDriveConfig(\n",
    "    run_config=script_runconfig, \n",
    "    #estimator=estimator,\n",
    "    \n",
    "    hyperparameter_sampling=param_sampling, \n",
    "    policy=early_termination_policy,\n",
    "    \n",
    "    # Here the primary metric is the label of one of logged metrics in the training run\n",
    "    # So, in order to use HyperDrive you MUST log at least one metric and use it as parameter\n",
    "    primary_metric_name='ROC-AUC',\n",
    "    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "    max_total_runs=20,\n",
    "    max_concurrent_runs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lauch the hyperparameter tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the HyperDrive run\n",
    "hyperdrive_run = experiment.submit(hyperdrive_config)\n",
    "\n",
    "# Check here how to submit the hyperdrive run as a step of an AML Pipeline:\n",
    "# https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(hyperdrive_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try now with the Bayesian Parameter Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_bayes_sampling = BayesianParameterSampling( {\n",
    "    \"--C\": uniform(0.0, 1.0),\n",
    "    \"--solver\": choice('newton-cg'),\n",
    "    \"--penalty\": choice('none', 'l2')\n",
    "    }\n",
    ")\n",
    "\n",
    "hyperdrive_bayes_config = HyperDriveConfig(\n",
    "    run_config=script_runconfig, \n",
    "    #estimator=estimator,\n",
    "    \n",
    "    hyperparameter_sampling=param_bayes_sampling, \n",
    "    \n",
    "    # No early termination is allowed when using the bayesian parameter sampling\n",
    "    #policy=early_termination_policy,\n",
    "    \n",
    "    # Here the primary metric is the label of one of logged metrics in the training run\n",
    "    # So, in order to use HyperDrive you MUST log at least one metric and use it as parameter\n",
    "    primary_metric_name='ROC-AUC',\n",
    "    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "    max_total_runs=20,\n",
    "    max_concurrent_runs=4)\n",
    "\n",
    "hyperdrive_bayes_run = experiment.submit(hyperdrive_bayes_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(hyperdrive_bayes_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_bayes_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Let's try now with the Bayesian Parameter Sampling for just the ElasticNet solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_elasticnet_bayes_sampling = BayesianParameterSampling( {\n",
    "    \"--C\": uniform(0.0, 1.0),\n",
    "    \"--solver\": choice('saga'),\n",
    "    \"--penalty\": choice('elasticnet'),\n",
    "    \"--l1_ratio\": uniform(0.0, 1.0)\n",
    "    }\n",
    ")\n",
    "\n",
    "hyperdrive_elasticnet_bayes_config = HyperDriveConfig(\n",
    "    run_config=script_runconfig, \n",
    "    #estimator=estimator,\n",
    "    \n",
    "    hyperparameter_sampling=param_elasticnet_bayes_sampling, \n",
    "    \n",
    "    # No early termination is allowed when using the bayesian parameter sampling\n",
    "    #policy=early_termination_policy,\n",
    "    \n",
    "    # Here the primary metric is the label of one of logged metrics in the training run\n",
    "    # So, in order to use HyperDrive you MUST log at least one metric and use it as parameter\n",
    "    primary_metric_name='ROC-AUC',\n",
    "    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "    max_total_runs=24,\n",
    "    max_concurrent_runs=4)\n",
    "\n",
    "hyperdrive_elasticnet_bayes_run = experiment.submit(hyperdrive_elasticnet_bayes_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(hyperdrive_elasticnet_bayes_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_elasticnet_bayes_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and get the best model found by HyperDrive¶ \n",
    "\n",
    "When all jobs finish, we can find out the one that has the highest accuracy. Let's get the best model from the *HyperDrive-RandomGrid-BanditPolicy* run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "print(best_run.get_details()['runDefinition']['arguments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy 'best_run' to 'run' to re-use the same code also used without HyperDrive\n",
    "run = best_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display run metrics results\n",
    "You now have a model trained on a remote cluster. Retrieve the accuracy of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run.get_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See files associated with the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run.get_file_names())\n",
    "\n",
    "run.download_file('azureml-logs/70_driver_log.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the model\n",
    "Once you've trained the model, you can save and register it to your workspace. Model registration lets you store and version your models in your workspace to simplify model management and deployment.\n",
    "\n",
    "A registered model is a logical container for one or more files that make up your model. For example, if you have a model that's stored in multiple files, you can register them as a single model in the workspace. After you register the files, you can then download or deploy the registered model.\n",
    "\n",
    "With the Model class, you can package models for use with Docker and deploy them as a real-time endpoint that can be used for inference requests.\n",
    "\n",
    "Running the following code will register the model to your workspace, and will make it available to reference by name in remote compute contexts or deployment scripts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all, download the traind model from the best HyperDrive run\n",
    "run.download_file('outputs/classif-empl-attrition.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model_name = 'aml-wrkshp-classif-empl-attrition'\n",
    "\n",
    "model_reg = run.register_model(\n",
    "    model_name=model_name,  # Name of the registered model in your workspace.\n",
    "    description='Binary classification model for employees attrition',\n",
    "    model_path='outputs/classif-empl-attrition.pkl', # Local file to upload and register as a model.\n",
    "    model_framework=Model.Framework.SCIKITLEARN,     # Framework used to create the model. Supported frameworks: TensorFlow, ScikitLearn, Onnx, Custom\n",
    "    model_framework_version='0.23.2',                # Version of scikit-learn used to create the model.\n",
    "    tags={'ml-task': \"binary-classification\", 'business-area': \"HR\"},\n",
    "    properties={'joblib-version': \"0.14.1\", 'pandas-version': \"1.0.0\"},\n",
    "    sample_input_dataset=aml_dataset\n",
    ")\n",
    "\n",
    "model_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to download Scikit-Learn model pickle file from the model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Model.get_model_path(model_name, _workspace=ws))\n",
    "\n",
    "model_from_registry = Model(ws, model_name)\n",
    "model_from_registry.download(target_dir='.', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try model predictions in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the model into memory\n",
    "model = joblib.load('classif-empl-attrition.pkl')\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and test datasets from .pkl files\n",
    "\n",
    "# Download the test datasets to local\n",
    "run.download_file('outputs/x_test.pkl')\n",
    "run.download_file('outputs/y_test.pkl')\n",
    "\n",
    "# Load the test datasets into memory\n",
    "x_test = joblib.load('x_test.pkl')\n",
    "y_test = joblib.load('y_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions and calculate Accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# Make Multiple Predictions\n",
    "y_predictions = model.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_predictions)\n",
    "rocauc = roc_auc_score(y_test, y_predictions)\n",
    "average_precision = average_precision_score(y_test, y_predictions)\n",
    "\n",
    "model_details_df = pd.DataFrame([accuracy, rocauc, average_precision],\n",
    "                                columns = ['SVM'],\n",
    "                                index=['Accuracy','ROC-AUC','Avg Precision'])\n",
    "\n",
    "model_details_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "class_names = y_test.unique()\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [(\"Confusion matrix, without normalization\", None),\n",
    "                  (\"Normalized confusion matrix\", 'true')]\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(model, x_test, y_test,\n",
    "                                 display_labels=class_names,\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the instance you want to use as input for a prediction\n",
    "instance_num = 6 # The seventh instance from the beginning (0-based)\n",
    "\n",
    "# Get the prediction for the upon defined index\n",
    "prediction_values = model.predict(x_test)\n",
    "prediction_probs = model.predict_proba(x_test)\n",
    "\n",
    "print(\"Classes:\")\n",
    "print(model.classes_)\n",
    "\n",
    "print(\"Prediction label for instance\", instance_num, \":\")\n",
    "print(prediction_values[instance_num])\n",
    "\n",
    "print(\"True label for instance\", instance_num, \":\")\n",
    "print(y_test.values[instance_num])\n",
    "\n",
    "print(\"Prediction probabilities for instance\", instance_num, \":\")\n",
    "print(prediction_probs[instance_num])\n",
    "\n",
    "x_test.iloc[instance_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "azureml_tutorial",
   "language": "python",
   "name": "azureml_tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
