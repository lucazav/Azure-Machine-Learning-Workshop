{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying a web service to Azure Container Instance (ACI)\n",
    "\n",
    "This notebook shows the steps for deploying a model as service to ACI. The workflow is similar no matter where you deploy your model:\n",
    "\n",
    "1. Register the model.\n",
    "2. Prepare to deploy. (Specify assets, usage, compute target.)\n",
    "3. Deploy the model to the compute target.\n",
    "4. Test the deployed model, also called a web service.\n",
    "5. Consume the model using Power BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core.compute import AksCompute, ComputeTarget\n",
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "from azureml.core.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.17.0\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "print(azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get workspace\n",
    "Load existing workspace from the config file info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo-ent-ws\n",
      "demo\n",
      "westeurope\n",
      "bcbf34a7-1936-4783-8840-8f324c37f354\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get or Register the model\n",
    "If not already done, register an existing trained model, add description and tags.\n",
    "\n",
    "This is the model you've already trained using manual training or using [Automated Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-create-portal-experiments).\n",
    "\n",
    "In the code snippet below we're using the already trained model original_model.pkl that is saved in the folder that contains this notebook. We're registering this model with the name \"IBM-attrition-model\". Later on we will use the same name in the scoring script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: aml-wrkshp-classif-empl-attrition\n",
      "Model description: Binary classification model for employees attrition\n",
      "Model version: 3\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "# if the model is already registered as part of training then uncomment the line below. Make sure model is registered with the name \"IBM_attrition_model\"\n",
    "model = Model(ws, 'aml-wrkshp-classif-empl-attrition')\n",
    "\n",
    "#Register the model\n",
    "# # if the model is not already registered as part of training register the original_model.pkl file provided in the same folder as this notebook\n",
    "# model = Model.register(model_path = \"original_model.pkl\", # this points to a local file\n",
    "#                        model_name = \"IBM_attrition_model\", # this is the name the model is registered as\n",
    "#                        tags = {'area': \"HR\", 'type': \"attrition\"},\n",
    "#                        description = \"Attrition model to understand attrition risk\",\n",
    "#                        workspace = ws)\n",
    "\n",
    "print('Model name: ', model.name, '\\n', 'Model description: ', model.description, '\\n', 'Model version: ', model.version, sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classif-empl-attrition.pkl': 'https://demoentws5367325393.blob.core.windows.net/azureml/ExperimentRun/dcid.HD_b644d7b0-1449-4a4e-8b77-f5a45df2828e_7/outputs/classif-empl-attrition.pkl?sv=2019-02-02&sr=b&sig=xHNJS6N0IaOQO5qwxVlOHskUJb5lIKVwX3StQduROqE%3D&st=2020-11-12T09%3A03%3A45Z&se=2020-11-12T17%3A13%3A45Z&sp=r'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Name of the sotred model as artifact\n",
    "model.get_sas_urls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare to deploy\n",
    "\n",
    "To deploy the model, you need the following items:\n",
    "\n",
    "- **An entry script**, this script accepts requests, scores the requests by using the model, and returns the results.\n",
    "- **Dependencies**, like helper scripts or Python/Conda packages required to run the entry script or model.\n",
    "- **The deployment configuration** for the compute target that hosts the deployed model. This configuration describes things like memory and CPU requirements needed to run the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define your entry script and dependencies\n",
    "\n",
    "### Entry script\n",
    "\n",
    "We will first write the entry script as shown below. Note a few points in the entry script.\n",
    "\n",
    "The script contains two functions that load and run the model:\n",
    "\n",
    "**init()**: Typically, this function loads the model into a global object. This function is run only once, when the Docker container for your web service is started.\n",
    "\n",
    "When you register a model, you provide a model name that's used for managing the model in the registry. You use this name with the Model.get_model_path() method to retrieve the path of the model file or files on the local file system. If you register a folder or a collection of files, this API returns the path of the directory that contains those files.\n",
    "\n",
    "**run(input_data)**: This function uses the model to predict a value based on the input data. Inputs and outputs of the run typically use JSON for serialization and deserialization. You can also work with raw binary data. You can transform the data before sending it to the model or before returning it to the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23.\n",
    "#Please import this functionality directly from joblib, which can be installed with: pip install joblib.\n",
    "\n",
    "#from sklearn.externals import joblib\n",
    "import joblib\n",
    "\n",
    "from inference_schema.schema_decorators import input_schema, output_schema\n",
    "from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n",
    "from inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n",
    "\n",
    "\n",
    "input_sample = pd.DataFrame(data=[{'Age': 41, 'BusinessTravel': 'Travel_Rarely', 'DailyRate': 1102, 'Department': 'Sales', 'DistanceFromHome': 1, 'Education': 2, 'EducationField': 'Life Sciences', 'EnvironmentSatisfaction': 2, 'Gender': 'Female', 'HourlyRate': 94, 'JobInvolvement': 3, 'JobLevel': 2, 'JobRole': 'Sales Executive', 'JobSatisfaction': 4, 'MaritalStatus': 'Single', 'MonthlyIncome': 5993, 'MonthlyRate': 19479, 'NumCompaniesWorked': 8, 'OverTime': 0, 'PercentSalaryHike': 11, 'PerformanceRating': 3, 'RelationshipSatisfaction': 1, 'StockOptionLevel': 0, 'TotalWorkingYears': 8, 'TrainingTimesLastYear': 0, 'WorkLifeBalance': 1, 'YearsAtCompany': 6, 'YearsInCurrentRole': 4, 'YearsSinceLastPromotion': 0, 'YearsWithCurrManager': 5}])\n",
    "output_sample = np.array([0])\n",
    "\n",
    "\n",
    "def init():\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment. Join this path with the filename of the model file.\n",
    "    # It holds the path to the directory that contains the deployed model (./azureml-models/$MODEL_NAME/$VERSION).\n",
    "    # If there are multiple models, this value is the path to the directory containing all deployed models (./azureml-models).\n",
    "    global model\n",
    "    \n",
    "    model_path = os.getenv('AZUREML_MODEL_DIR')\n",
    "    if (model_path is None):\n",
    "        model_path = '.'\n",
    "    \n",
    "    model_path = os.path.join(model_path, 'classif-empl-attrition.pkl')\n",
    "    print(model_path)\n",
    "    \n",
    "    # Deserialize the model file back into a sklearn model\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "\n",
    "@input_schema('data', PandasParameterType(input_sample))\n",
    "@output_schema(NumpyParameterType(output_sample))\n",
    "def run(data):\n",
    "    try:\n",
    "        result = model.predict(data)\n",
    "        return json.dumps({\"result\": result.tolist()})\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        return json.dumps({\"error\": result})\n",
    "    \n",
    "# Test the functions if run locally\n",
    "if __name__ == \"__main__\":\n",
    "    init()\n",
    "    \n",
    "    prediction = run(input_sample)\n",
    "\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic schema generation\n",
    "To automatically generate a schema for your web service, provide a sample of the input and/or output in the constructor for one of the defined type objects. The type and sample are used to automatically create the schema. Azure Machine Learning then creates an OpenAPI (Swagger) specification for the web service during deployment.\n",
    "To use schema generation, include the _inference-schema_ package in your Conda environment file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dependencies\n",
    "\n",
    "The following YAML is the Conda dependencies file we will use for inference. If you want to use automatic schema generation, your entry script must import the inference-schema packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting myenv.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile myenv.yml\n",
    "\n",
    "name: project_environment\n",
    "\n",
    "dependencies:\n",
    "- python=3.6.2\n",
    "- pip:\n",
    "  - azureml-core==1.17.0\n",
    "  - azureml-defaults==1.17.0\n",
    "  - scikit-learn==0.22.2.post1\n",
    "  - sklearn-pandas\n",
    "  - inference-schema[numpy-support]\n",
    "- pandas\n",
    "- numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "# Instantiate environment\n",
    "myenv = Environment.from_conda_specification(name = \"myenv\",\n",
    "                                             file_path = \"myenv.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define your inference configuration\n",
    "\n",
    "The inference configuration describes how to configure the model to make predictions. This configuration isn't part of your entry script. It references your entry script and is used to locate all the resources required by the deployment. It's used later, when you deploy the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(entry_script='score.py', environment=myenv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define your deployment configuration\n",
    "\n",
    "Before deploying your model, you must define the deployment configuration. The deployment configuration is specific to the compute target that will host the web service. The deployment configuration isn't part of your entry script. It's used to define the characteristics of the compute target that will host the model and entry script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags = {'area': \"HR\", 'type': \"attrition\"}, \n",
    "                                               description='Explain predictions on employee attrition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Deploy Model as Webservice on Azure Container Instance\n",
    "\n",
    "Deployment uses the inference configuration deployment configuration to deploy the models. The deployment process is similar regardless of the compute target.\n",
    "\n",
    "In summary, a deployed service is created from a model, script, and associated files. The resulting web service is a load-balanced, HTTP endpoint with a REST API. You can send data to this API and receive the prediction returned by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The web service 'predict-attrition' has been deleted.\n"
     ]
    }
   ],
   "source": [
    "# Delete web service if already exists\n",
    "webservice_name = 'predict-attrition'\n",
    "\n",
    "try:\n",
    "    service = Webservice(name=webservice_name, workspace=ws)\n",
    "    service.delete()\n",
    "    \n",
    "    print(\"The web service '\", webservice_name, \"' has been deleted.\", sep='')\n",
    "except Exception as e:\n",
    "    if (e.args[0].split(':', 1)[0] == 'WebserviceNotFound'):\n",
    "        print(\"The web service '\", webservice_name, \"' doesn't exist.\", sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running...................................................................................\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "service = Model.deploy(ws,\n",
    "                       name=webservice_name,\n",
    "                       models=[model],\n",
    "                       inference_config=inference_config,\n",
    "                       deployment_config=aciconfig)\n",
    "\n",
    "service.wait_for_deployment(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Healthy\n"
     ]
    }
   ],
   "source": [
    "print(service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-12T13:28:13.8748471Z stdout F 2020-11-12T13:28:13,867933000+00:00 - gunicorn/run \n",
      "2020-11-12T13:28:13.8748471Z stdout F 2020-11-12T13:28:13,866875800+00:00 - iot-server/run \n",
      "2020-11-12T13:28:13.8798434Z stdout F 2020-11-12T13:28:13,878948800+00:00 - nginx/run \n",
      "2020-11-12T13:28:13.893853Z stdout F 2020-11-12T13:28:13,876473000+00:00 - rsyslog/run \n",
      "2020-11-12T13:28:13.9328664Z stderr F /usr/sbin/nginx: /azureml-envs/azureml_372ee22c7114cd6a865677362c842316/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "2020-11-12T13:28:13.9328664Z stderr F /usr/sbin/nginx: /azureml-envs/azureml_372ee22c7114cd6a865677362c842316/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "2020-11-12T13:28:13.9388589Z stderr F /usr/sbin/nginx: /azureml-envs/azureml_372ee22c7114cd6a865677362c842316/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "2020-11-12T13:28:13.9388589Z stderr F /usr/sbin/nginx: /azureml-envs/azureml_372ee22c7114cd6a865677362c842316/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "2020-11-12T13:28:13.9388589Z stderr F /usr/sbin/nginx: /azureml-envs/azureml_372ee22c7114cd6a865677362c842316/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "2020-11-12T13:28:15.4832854Z stdout F EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "2020-11-12T13:28:15.4901908Z stdout F 2020-11-12T13:28:15,489132300+00:00 - iot-server/finish 1 0\n",
      "2020-11-12T13:28:15.4966298Z stdout F 2020-11-12T13:28:15,493638900+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "2020-11-12T13:28:15.6848913Z stdout F Starting gunicorn 19.9.0\n",
      "2020-11-12T13:28:15.6858906Z stdout F Listening at: http://127.0.0.1:31311 (67)\n",
      "2020-11-12T13:28:15.6858906Z stdout F Using worker: sync\n",
      "2020-11-12T13:28:15.687012Z stdout F worker timeout is set to 300\n",
      "2020-11-12T13:28:15.7189093Z stdout F Booting worker with pid: 96\n",
      "2020-11-12T13:28:21.8131451Z stdout F SPARK_HOME not set. Skipping PySpark Initialization.\n",
      "2020-11-12T13:28:21.8131451Z stdout F Initializing logger\n",
      "2020-11-12T13:28:21.8131451Z stdout F 2020-11-12 13:28:21,810 | root | INFO | Starting up app insights client\n",
      "2020-11-12T13:28:21.8131451Z stdout F Starting up app insights client\n",
      "2020-11-12T13:28:21.8131451Z stdout F 2020-11-12 13:28:21,811 | root | INFO | Starting up request id generator\n",
      "2020-11-12T13:28:21.8131451Z stdout F Starting up request id generator\n",
      "2020-11-12T13:28:21.8131451Z stdout F 2020-11-12 13:28:21,811 | root | INFO | Starting up app insight hooks\n",
      "2020-11-12T13:28:21.8131451Z stdout F Starting up app insight hooks\n",
      "2020-11-12T13:28:21.8141502Z stdout F 2020-11-12 13:28:21,812 | root | INFO | Invoking user's init function\n",
      "2020-11-12T13:28:21.8141502Z stdout F Invoking user's init function\n",
      "2020-11-12T13:28:21.9480299Z stdout F azureml-models/aml-wrkshp-classif-empl-attrition/3/classif-empl-attrition.pkl\n",
      "2020-11-12T13:28:21.9480299Z stdout F 2020-11-12 13:28:21,945 | root | INFO | Users's init has completed successfully\n",
      "2020-11-12T13:28:21.9480299Z stdout F Users's init has completed successfully\n",
      "2020-11-12T13:28:21.9492081Z stdout F 2020-11-12 13:28:21,956 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\n",
      "2020-11-12T13:28:21.9492081Z stdout F Skipping middleware: dbg_model_info as it's not enabled.\n",
      "2020-11-12T13:28:21.9492081Z stdout F 2020-11-12 13:28:21,956 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\n",
      "2020-11-12T13:28:21.9492081Z stdout F Skipping middleware: dbg_resource_usage as it's not enabled.\n",
      "2020-11-12T13:28:21.9614378Z stdout F 2020-11-12 13:28:21,959 | root | INFO | Scoring timeout is found from os.environ: 60000 ms\n",
      "2020-11-12T13:28:21.9614378Z stdout F Scoring timeout is found from os.environ: 60000 ms\n",
      "2020-11-12T13:28:23.6631343Z stdout F 2020-11-12 13:28:23,661 | root | INFO | 200\n",
      "2020-11-12T13:28:23.6631343Z stdout F 200\n",
      "2020-11-12T13:28:23.6641332Z stdout F 127.0.0.1 - - [12/Nov/2020:13:28:23 +0000] \"GET /swagger.json HTTP/1.0\" 200 4303 \"-\" \"Go-http-client/1.1\"\n",
      "2020-11-12T13:28:28.2772385Z stdout F 2020-11-12 13:28:28,274 | root | INFO | 200\n",
      "2020-11-12T13:28:28.2772385Z stdout F 200\n",
      "2020-11-12T13:28:28.2792187Z stdout F 127.0.0.1 - - [12/Nov/2020:13:28:28 +0000] \"GET /swagger.json HTTP/1.0\" 200 4303 \"-\" \"Go-http-client/1.1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In case of deploying error, debug using the logs\n",
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web service schema\n",
    "\n",
    "If you used automatic schema generation with your deployment, you can get the address of the OpenAPI specification for the service by using the swagger_uri property. (For example, print(service.swagger_uri).) Use a GET request or open the URI in a browser to retrieve the specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://783095e2-403b-4910-83d0-8575207ec279.westeurope.azurecontainer.io/swagger.json\n"
     ]
    }
   ],
   "source": [
    "print(service.swagger_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the deployed model\n",
    "\n",
    "Every deployed web service provides a REST API, so you can create client applications in a variety of programming languages. If you've enabled key authentication for your service, you need to provide a service key as a token in your request header. If you've enabled token authentication for your service, you need to provide an Azure Machine Learning JWT token as a bearer token in your request header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"result\": [1]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# the sample below contains the data for an employee that is not an attrition risk\n",
    "sample = pd.DataFrame(data=[{'Age': 49, 'BusinessTravel': 'Travel_Rarely', 'DailyRate': 1098, 'Department': 'Research & Development', 'DistanceFromHome': 4, 'Education': 2, 'EducationField': 'Medical', 'EnvironmentSatisfaction': 4, 'Gender': 'Female', 'HourlyRate': 21, 'JobInvolvement': 3, 'JobLevel': 2, 'JobRole': 'Laboratory Technician', 'JobSatisfaction': 3, 'MaritalStatus': 'Single', 'MonthlyIncome': 711, 'MonthlyRate': 2124, 'NumCompaniesWorked': 8, 'OverTime': 1, 'PercentSalaryHike': 8, 'PerformanceRating': 4, 'RelationshipSatisfaction': 3, 'StockOptionLevel': 0, 'TotalWorkingYears': 2, 'TrainingTimesLastYear': 0, 'WorkLifeBalance': 3, 'YearsAtCompany': 2, 'YearsInCurrentRole': 1, 'YearsSinceLastPromotion': 0, 'YearsWithCurrManager': 1}])\n",
    "\n",
    "# the sample below contains the data for an employee that is an attrition risk\n",
    "# sample = pd.DataFrame(data=[{'Age': 49, 'BusinessTravel': 'Travel_Rarely', 'DailyRate': 1098, 'Department': 'Research & Development', 'DistanceFromHome': 4, 'Education': 2, 'EducationField': 'Medical', 'EnvironmentSatisfaction': 4, 'Gender': 'Female', 'HourlyRate': 21, 'JobInvolvement': 3, 'JobLevel': 2, 'JobRole': 'Laboratory Technician', 'JobSatisfaction': 3, 'MaritalStatus': 'Single', 'MonthlyIncome': 711, 'MonthlyRate': 2124, 'NumCompaniesWorked': 8, 'OverTime': 'Yes', 'PercentSalaryHike': 8, 'PerformanceRating': 4, 'RelationshipSatisfaction': 3, 'StockOptionLevel': 0, 'TotalWorkingYears': 2, 'TrainingTimesLastYear': 0, 'WorkLifeBalance': 3, 'YearsAtCompany': 2, 'YearsInCurrentRole': 1, 'YearsSinceLastPromotion': 0, 'YearsWithCurrManager': 1}])\n",
    "\n",
    "\n",
    "# converts the sample to JSON string\n",
    "sample = pd.DataFrame.to_json(sample)\n",
    "\n",
    "# deserializes sample to a python object \n",
    "sample = json.loads(sample)\n",
    "\n",
    "# serializes sample to JSON formatted string as expected by the scoring script\n",
    "sample = json.dumps({\"data\":sample})\n",
    "\n",
    "prediction = service.run(sample)\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consume the model using Power BI\n",
    "You can also consume the model from Power BI. See details [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-consume-web-service#consume-the-service-from-power-bi).\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
